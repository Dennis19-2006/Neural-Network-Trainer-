import streamlit as st
import numpy as np
import pandas as pd
import time
from interpreter_bot import InterpreterBot, create_interpreter_bot

# -----------------------------
# Neural network logic (yours)
# -----------------------------
def sigmoid(x):
    x = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def train_network(X, Y, lr, epochs, hidden_neurons):
    np.random.seed(42)

    num_inputs = X.shape[1]
    num_outputs = 1

    W_hidden = np.random.randn(num_inputs, hidden_neurons) * np.sqrt(2.0 / num_inputs)
    W_output = np.random.randn(hidden_neurons, num_outputs) * np.sqrt(2.0 / hidden_neurons)
    B_hidden = np.zeros((1, hidden_neurons))
    B_output = np.zeros((1, num_outputs))

    error_history = []
    activations_snapshots = []

    progress = st.progress(0)
    status = st.empty()

    for epoch in range(epochs):
        total_error = 0
        indices = np.random.permutation(len(X))

        for idx in indices:
            x = X[idx:idx+1]
            y = Y[idx:idx+1]

            # Forward
            Z_hidden = np.dot(x, W_hidden) + B_hidden
            A_hidden = sigmoid(Z_hidden)
            Z_output = np.dot(A_hidden, W_output) + B_output
            A_output = sigmoid(Z_output)

            error = y - A_output
            total_error += error[0, 0] ** 2

            # Backprop
            dA_output = -2 * error
            dZ_output = dA_output * sigmoid_derivative(A_output)
            dW_output = np.dot(A_hidden.T, dZ_output)
            dB_output = dZ_output

            dA_hidden = np.dot(dZ_output, W_output.T)
            dZ_hidden = dA_hidden * sigmoid_derivative(A_hidden)
            dW_hidden = np.dot(x.T, dZ_hidden)
            dB_hidden = dZ_hidden

            # Update
            W_output -= lr * np.clip(dW_output, -1, 1)
            B_output -= lr * np.clip(dB_output, -1, 1)
            W_hidden -= lr * np.clip(dW_hidden, -1, 1)
            B_hidden -= lr * np.clip(dB_hidden, -1, 1)

        avg_error = total_error / len(X)
        error_history.append(avg_error)

        activations_snapshots.append({
            "hidden": A_hidden[0].tolist(),
            "output": float(A_output[0, 0])
        })

        progress.progress((epoch + 1) / epochs)
        status.text(f"Epoch {epoch+1}/{epochs}  |  Error: {avg_error:.6f}")
        time.sleep(0.01)

    return {
        "W_hidden": W_hidden,
        "W_output": W_output,
        "B_hidden": B_hidden,
        "B_output": B_output,
        "errors": error_history,
        "activations": activations_snapshots
    }

def predict(x, model):
    Z_hidden = np.dot(x, model["W_hidden"]) + model["B_hidden"]
    A_hidden = sigmoid(Z_hidden)
    Z_output = np.dot(A_hidden, model["W_output"]) + model["B_output"]
    return sigmoid(Z_output)

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="Neural Network Trainer", layout="wide")
st.title("ğŸ§  Neural Network Trainer (Streamlit)")

# Create sample data if no file uploaded
sample_data = None
try:
    sample_data = pd.read_csv("frontend/sample_data.csv")
except:
    # Generate synthetic data if sample_data.csv doesn't exist
    np.random.seed(42)
    X_sample = np.random.randn(100, 2)
    y_sample = (X_sample[:, 0] + X_sample[:, 1] > 0).astype(int)
    sample_data = pd.DataFrame(np.column_stack([X_sample, y_sample]), columns=["Feature1", "Feature2", "Target"])

# Initialize interpreter bot
if "interpreter_bot" not in st.session_state:
    st.session_state["interpreter_bot"] = create_interpreter_bot()

# =====================================================================
# HELPER FUNCTION: Generate Full Dataset Assessment
# =====================================================================
def generate_dataset_assessment(df, bot):
    """Generate comprehensive assessment of dataset"""
    st.markdown("---")
    st.markdown("### ğŸ“Š Dataset Assessment Report")
    
    # Dataset statistics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Samples", len(df))
    with col2:
        st.metric("Features", len(df.columns) - 1)
    with col3:
        st.metric("Target Type", "Continuous" if df.iloc[:, -1].dtype in ['float64', 'float32'] else "Categorical")
    with col4:
        st.metric("Data Quality", f"{(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.1f}%")
    
    # Feature analysis
    with st.expander("ğŸ” Feature Analysis", expanded=True):
        feature_cols = df.columns[:-1]
        for col in feature_cols:
            col_data = df[col].describe()
            st.write(f"**{col}**: Î¼={col_data['mean']:.4f}, Ïƒ={col_data['std']:.4f}, range=[{col_data['min']:.2f}, {col_data['max']:.2f}]")
    
    # Target analysis
    with st.expander("ğŸ¯ Target Distribution", expanded=True):
        target_col = df.columns[-1]
        st.write(f"**{target_col} Statistics:**")
        st.write(df[target_col].describe())
        
        if len(df[target_col].unique()) <= 10:
            st.write("**Value Counts:**")
            st.write(df[target_col].value_counts())
    
    st.markdown("---")

# =====================================================================
# HELPER FUNCTION: Generate Training Assessment
# =====================================================================
def generate_training_assessment(model, X, Y, bot):
    """Generate comprehensive assessment after training"""
    st.markdown("---")
    st.markdown("### ğŸ“ˆ Training Assessment Report")
    
    # Make predictions on training data
    predictions = predict(X, model)
    predictions_flat = predictions.flatten()
    Y_flat = Y.flatten()
    
    # Calculate detailed metrics
    mae = np.mean(np.abs(predictions_flat - Y_flat))
    mse = np.mean((predictions_flat - Y_flat) ** 2)
    rmse = np.sqrt(mse)
    
    # Calculate true confidence metrics
    mean_pred = np.mean(predictions_flat)
    ss_tot = np.sum((Y_flat - mean_pred) ** 2)
    ss_res = np.sum((Y_flat - predictions_flat) ** 2)
    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    # Calculate prediction spread (how confident/concentrated)
    pred_std = np.std(predictions_flat)
    actual_std = np.std(Y_flat)
    
    # Calculate error distribution
    errors = np.abs(predictions_flat - Y_flat)
    error_mean = np.mean(errors)
    error_std = np.std(errors)
    within_1_std = np.sum(errors <= error_mean + error_std) / len(errors) * 100
    within_2_std = np.sum(errors <= error_mean + 2*error_std) / len(errors) * 100
    
    # Calibration: how well predictions match actual distribution
    calibration_error = abs(np.mean(predictions_flat) - np.mean(Y_flat))
    
    # Real confidence score (inverse of error)
    confidence_score = max(0, 1 - mae)
    confidence_pct = confidence_score * 100
    
    # Display confidence metrics
    st.markdown("### ğŸ¯ Real Confidence Assessment")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Confidence Score", f"{confidence_pct:.1f}%", "â†‘ Higher is better")
    with col2:
        st.metric("RÂ² Score", f"{r_squared:.4f}", "â†‘ Closer to 1.0 is better")
    with col3:
        st.metric("MAE", f"{mae:.4f}", "â†“ Lower is better")
    with col4:
        st.metric("RMSE", f"{rmse:.4f}", "â†“ Lower is better")
    
    # Confidence interpretation
    if confidence_pct >= 95:
        conf_text = "ğŸŸ¢ **EXTREMELY HIGH** - Model is very precise and trustworthy"
    elif confidence_pct >= 85:
        conf_text = "ğŸŸ¢ **VERY HIGH** - Model predictions are highly reliable"
    elif confidence_pct >= 75:
        conf_text = "ğŸŸ¡ **HIGH** - Model is reasonably confident, generally trustworthy"
    elif confidence_pct >= 65:
        conf_text = "ğŸŸ¡ **MODERATE** - Model has acceptable precision, use with caution"
    elif confidence_pct >= 50:
        conf_text = "ğŸŸ  **LOW** - Model predictions have high variability"
    else:
        conf_text = "ğŸ”´ **VERY LOW** - Model is not confident, unreliable predictions"
    
    st.markdown(f"**Confidence Assessment:** {conf_text}")
    
    st.markdown("---")
    
    # Detailed metrics
    with st.expander("ğŸ“Š Detailed Precision Metrics", expanded=True):
        st.write(f"""
        **Prediction Accuracy Metrics:**
        - **Mean Absolute Error (MAE):** {mae:.6f}
          - On average, predictions differ from actual by {mae*100:.2f}%
        
        - **Root Mean Squared Error (RMSE):** {rmse:.6f}
          - Typical prediction deviation: {rmse*100:.2f}%
        
        - **RÂ² Score:** {r_squared:.6f}
          - Explains {r_squared*100:.2f}% of variance in target
          - 1.0 = perfect fit, 0.0 = no correlation
        
        **Prediction Distribution:**
        - **Prediction Std Dev:** {pred_std:.6f}
          - Model confidence spread: {pred_std*100:.2f}%
        
        - **Actual Std Dev:** {actual_std:.6f}
          - Actual data variability: {actual_std*100:.2f}%
        
        - **Distribution Match:** {abs(pred_std - actual_std):.6f}
          - How well model captures data spread
        
        **Error Distribution:**
        - **Mean Error:** {error_mean:.6f}
        - **Error Std Dev:** {error_std:.6f}
        - **Predictions within 1Ïƒ:** {within_1_std:.1f}%
        - **Predictions within 2Ïƒ:** {within_2_std:.1f}%
        
        **Calibration:**
        - **Calibration Error:** {calibration_error:.6f}
          - Model mean vs actual mean difference
          - Lower is better (shows how well model centers predictions)
        """)
    
    # Performance insights
    with st.expander("ğŸ” Performance Insights", expanded=True):
        # Find best and worst predictions
        best_idx = np.argmin(errors)
        worst_idx = np.argmax(errors)
        
        st.write(f"""
        **âœ… Best Prediction:**
        - Predicted: {predictions_flat[best_idx]:.4f}
        - Actual: {Y_flat[best_idx]:.4f}
        - Error: {errors[best_idx]:.6f} ({errors[best_idx]*100:.2f}%)
        
        **âŒ Worst Prediction:**
        - Predicted: {predictions_flat[worst_idx]:.4f}
        - Actual: {Y_flat[worst_idx]:.4f}
        - Error: {errors[worst_idx]:.6f} ({errors[worst_idx]*100:.2f}%)
        
        **Error Range:**
        - Min Error: {np.min(errors):.6f}
        - Max Error: {np.max(errors):.6f}
        - Error Median: {np.median(errors):.6f}
        """)
        
        # Bias analysis
        mean_error_signed = np.mean(predictions_flat - Y_flat)
        if abs(mean_error_signed) < 0.01:
            bias_text = "âœ… **NO BIAS** - Predictions are well-calibrated"
        elif mean_error_signed > 0:
            bias_text = f"ğŸ“Š **OVERESTIMATION BIAS** - Tends to predict {mean_error_signed*100:.2f}% too high"
        else:
            bias_text = f"ğŸ“Š **UNDERESTIMATION BIAS** - Tends to predict {abs(mean_error_signed)*100:.2f}% too low"
        
        st.write(f"**Bias Analysis:** {bias_text}")
    
    # Error history visualization
    with st.expander("ğŸ“‰ Training Progress", expanded=True):
        st.line_chart(model["errors"])
        avg_error = np.mean(model["errors"])
        st.write(f"Average training error: {avg_error:.6f}")
    
    # Prediction vs Actual scatter plot
    with st.expander("ğŸ“ˆ Prediction Accuracy Visualization", expanded=True):
        try:
            import matplotlib.pyplot as plt
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            
            # Scatter plot: Predicted vs Actual
            ax1.scatter(Y_flat, predictions_flat, alpha=0.6, s=30)
            min_val = min(Y_flat.min(), predictions_flat.min())
            max_val = max(Y_flat.max(), predictions_flat.max())
            ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Fit')
            ax1.set_xlabel('Actual Values')
            ax1.set_ylabel('Predicted Values')
            ax1.set_title('Predictions vs Actual')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Error distribution
            ax2.hist(errors, bins=20, edgecolor='black', alpha=0.7)
            ax2.axvline(error_mean, color='r', linestyle='--', linewidth=2, label=f'Mean Error: {error_mean:.4f}')
            ax2.set_xlabel('Absolute Error')
            ax2.set_ylabel('Frequency')
            ax2.set_title('Error Distribution')
            ax2.legend()
            ax2.grid(True, alpha=0.3, axis='y')
            
            st.pyplot(fig)
            plt.close(fig)
        except Exception as e:
            st.warning(f"Could not display visualization: {str(e)}")
    
    st.markdown("---")

# =====================================================================
# DATA LOADING INTERFACE
# =====================================================================
tab1, tab2 = st.tabs(["ğŸ“Š Sample Data", "ğŸ“¤ Upload Data"])

with tab1:
    st.subheader("Sample Dataset")
    st.dataframe(sample_data.head(10))
    use_sample = st.button("Use Sample Data", key="sample_btn")
    if use_sample:
        st.session_state["data"] = sample_data
        st.success("Sample data loaded!")

with tab2:
    uploaded_file = st.file_uploader("Upload CSV dataset", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.dataframe(df.head(10))
        use_upload = st.button("Use Uploaded Data", key="upload_btn")
        if use_upload:
            st.session_state["data"] = df
            st.success("Data uploaded and loaded!")

# Process the selected data
if "data" in st.session_state:
    df = st.session_state["data"]
    bot = st.session_state["interpreter_bot"]
    
    st.markdown("## ğŸ“Š Loaded Dataset")
    st.dataframe(df.head())
    
    # Generate dataset assessment
    generate_dataset_assessment(df, bot)

    X = df.iloc[:, :-1].values.astype(np.float32)
    Y = df.iloc[:, -1].values.astype(np.float32).reshape(-1, 1)

    st.sidebar.header("âš™ï¸ Training Settings")
    lr = st.sidebar.slider("Learning Rate", 0.001, 1.0, 0.1)
    epochs = st.sidebar.slider("Epochs", 10, 2000, 500)
    hidden_neurons = st.sidebar.slider("Hidden Neurons", 1, 50, 10)

    if st.button("ğŸš€ Train Model", key="train_btn"):
        with st.spinner("Training neural network..."):
            model = train_network(X, Y, lr, epochs, hidden_neurons)
            st.session_state["model"] = model
            st.session_state["num_features"] = X.shape[1]

        st.success("âœ¨ Training complete!")
        
        # Generate training assessment
        generate_training_assessment(model, X, Y, bot)

# Prediction with Interpretation
if "model" in st.session_state:
    st.markdown("---")
    st.markdown("## ğŸ”® Make New Predictions")

    inputs = []
    for i in range(st.session_state["num_features"]):
        val = st.number_input(f"Feature {i+1}", value=0.0)
        inputs.append(val)

    if st.button("ğŸš€ Get Prediction", key="predict_btn"):
        x = np.array(inputs).reshape(1, -1)
        pred = predict(x, st.session_state["model"])
        pred_value = float(pred[0, 0])
        st.session_state["last_prediction"] = pred_value
        
        bot = st.session_state["interpreter_bot"]
        
        # Show prediction value
        st.markdown("---")
        st.markdown("### ğŸ¯ Prediction Result")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Raw Prediction", f"{pred_value:.4f}")
        with col2:
            st.metric("Percentage", f"{pred_value*100:.1f}%")
        with col3:
            # Prediction confidence based on distance from 0.5
            distance_from_boundary = min(abs(pred_value - 0.5), 0.5)
            pred_confidence = (distance_from_boundary * 2) * 100  # 0-100%
            st.metric("Prediction Confidence", f"{pred_confidence:.1f}%")
        
        # Interpret prediction
        with st.expander("ğŸ“– What This Prediction Means", expanded=True):
            insight = bot.interpret_prediction(pred_value, context="neural network output")
            st.markdown(insight.user_friendly_description)
            
            with st.expander("ğŸ“Š Probability Details"):
                st.write(insight.probability_explanation)
            
            with st.expander("ğŸ’¡ Recommendations"):
                for rec in insight.recommendations:
                    st.write(f"â€¢ {rec}")
            
            with st.expander("ğŸ“ˆ Trend"):
                st.write(insight.trend_analysis)
        
        # Real confidence explanation
        with st.expander("ğŸ¯ Real Confidence Explained", expanded=True):
            st.write(f"""
            **Prediction Confidence: {pred_confidence:.1f}%**
            
            This measures how **far the prediction is from the uncertainty boundary** (0.5).
            
            - **0%** = Completely uncertain (prediction = 0.5)
            - **100%** = Maximum certainty (prediction = 0.0 or 1.0)
            
            **For this prediction:**
            - Value: {pred_value:.4f}
            - Distance from 0.5: {abs(pred_value - 0.5):.4f}
            - Confidence: {pred_confidence:.1f}%
            
            The higher this percentage, the more **decisive** the model is about its prediction.
            """)



